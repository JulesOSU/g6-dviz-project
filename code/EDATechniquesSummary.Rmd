---
title: "EDA Techniques Summary"
author: "Group 6"
date: "5/11/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(vcd)
library(effects)
```

## Section 7.3.1 Visualizing Categorical Distributions  

### doubledecker()  

**__doubledecker()__** is a mosaic plot that is used for categorical data  to visualize contingency tables. The plots offer a graphical way to display dependencies and associations among variables in a data set. The api is in the vcd package. For example, the Arrests data from Toronto, If you want to look at the influence of "citizen" and "colour" on being "released", you can use a doubledecker plot to do this.

**__doubledecker()__** varies from a standard mosaic in that you specify one dependent variable of interest which permutates the table and is the last dimension specified in the plot. Here's an example of the generated plot.  

```{r sect7-3, echo=TRUE, include=TRUE}
dat <- Arrests
doubledecker(released ~ citizen +colour, data=Arrests, gp =( gpar(fill = c("orange", "blue"))), main=TRUE)


```
Using gp in this context applies the supplied vars to only the dependent variable.  

### assoc ()  

Another useful mosaic plot for visualizing associations is **__asssoc()__**. The area of the box is proportional to the difference in observed and expected frequencies. The placement of the box is relative to the baseline for independence (0), so if it's greater than the baseline, it's above the line, otherwise its below the line. 

```{r sect7-3b, echo=TRUE, include=TRUE}

assoc(released ~ citizen + colour, data=Arrests, shade=TRUE, main=TRUE, comnpress = FALSE)
```

## Covariation of a Categorical and a Continuous Variable:

The book gave two examples of how to deal with a categorical and continuous variable that have covariation. One technique that was given was comparing the **density** of variables rather than the **count**. This can lead to more interpretable visualization when the counts across categorical variables are not very even. The below plots compare the same data of the diamonds data set, but rather than looking at count the second plot looks at density. You can see that colors H and I and J appear to more regulary have diamonds in a higher price point. 


```{r density}
ggplot(data = diamonds, mapping = aes(x = price)) + 
  geom_freqpoly(mapping = aes(colour = color), binwidth = 500)
ggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) + 
  geom_freqpoly(mapping = aes(colour = color), binwidth = 500)
```


The freq_poly plots produced 7 categories, making it difficult to read. Another example would be a box plot. The box shows the 25th, 50th and 75th percentile, with outliers as individual points and points outside of those percentiles as whiskers. Below it is much more clear that H I and J have higher average prices, but they also have wider spreads. 

```{r}
ggplot(data = diamonds, mapping = aes(x = color, y = price)) +
  geom_boxplot()
```

## 7.3.3 Unusual Values: 

During exploratory data analysis it's always a good idea to check for outliers or unusual observations. Sometimes outliers are errors in a dataset, and sometimes they are genuine observations that are just outside the normal bounds for a particular variable. Often times it is difficult to tell, so further exploration is generally required. Let's examine the distribution of diamond sizes by plotting a histogram of the y-dimension measured in mm. 

```{r}
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5, color = "white")
```

The unusually wide x-axis gives us reason to suspect that there may be a few outliers with counts so low that they don't even show up on a histogram. Change the scale of the y-axis to get a better look. 

```{r}
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5, color = "white") +
  coord_cartesian(ylim = c(0, 50))
```

We can see two outlying observations around 30 and 60, and a handful at 0. Let's examine these cases further. We'll look at the `x` and `z` variables as well to get a better idea of the actual sizes and we will include `price` to see if there is anything unusual there. 

```{r}
unusual <- diamonds %>% 
  filter(y < 3 | y > 20) %>% 
  select(price, x, y, z) %>%
  arrange(y)

unusual
```

It is impossible for a diamond to have dimensions of 0, so we can assume these are measurement errors or perhaps a coded value which means something else. The other two observations have more reasonable `x` and `z` values, so it seems reasonable to assume that these are measurement errors. Let's remove all the unusual observations by subsetting the data inside the `ggplot()` function, and see what the distribution looks like. 

```{r}
ggplot(diamonds[(diamonds$y < 20) & (diamonds$y > 0),]) + 
  geom_histogram(aes(y), binwidth = 0.05, color = "white")
```

By removing unusual observations we can get a much more detailed look at how the sizes of diamonds are distributed. I would guess the multiple peaks are due to jewelers rounding up to meet a particular size with a prespecified price point, but we never would have seen this interesting pattern without removing the outliers. 

## 7.5.2. Two Categorical Variables   
   
One technique to visualise the corvariation between two categorical variables. 
The book uses the diamond dataset. Two cateogrical variables in the example include cut and color. 

***Two Categorical Variables***
Cut = Fair, Good, Very Good, Premium, and Ideal
Color = J, I, H, G, F, E, D

To plot two categorical variables
We can count the number of observations for each combination. 
The book states - One way to do that is to reply on the built-in geom_count()


```{r}
library(dplyr)
library(ggplot2)
```   
   
Plot diamonds dataset. Geom count to cut and color 
   
```{r 1000}

ggplot(data = diamonds) +
  geom_count(mapping = aes(x = cut, y = color))

```   
   
The book describes another way to visualise by using geom_title and fill aesthetic.
The fill will be the total of teach each color to each cut 
   
```{r 1001}
diamonds %>% 
  count(color, cut) %>%  
  ggplot(mapping = aes(x = color, y = cut)) +
    geom_tile(mapping = aes(fill = n))

```   
   
We can see that the color G has the most number of ideal diamond cuts. 
F and H colors have a higher amount of ideal diamonds but as many as G. 

To the contrast, the fair cut has the about the same amount for all colors. As the cut grade goes from fair to ideal colors G, F, H, and E have more diamonds than colors D, E, I, and J.


## 7.5.3.1: Two Continuous Variables

#### Question 1 7.5.3.1 Exercises
Instead of summarising the conditional distribution with a boxplot, you could use a frequency polygon. What do you need to consider when using cut_width() vs cut_number()? How does that impact a visualisation of the 2d distribution of carat and price?

* Cut number puts the same number of points in each bin. Below, I have 20 points in each bin, and each boxplot shows a distribution based on that.

```{r continuous variables exercises - question 1}

# Cut width vs cut number comparison
 # cut number
ggplot(data = diamonds, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_number(carat, 20)))+
     theme_minimal() + 
   ggtitle("Cut Number")
# cut width
ggplot(data = diamonds, mapping = aes(x = carat, y = price)) + 
  geom_boxplot(mapping = aes(group = cut_width(carat, 1, varwidth = TRUE)))+
     theme_minimal()+ 
   ggtitle("Cut Width")

```
#### Question 2 Visualise the distribution of carat, partitioned by price. 7.5.3.1 Exercises

```{r continuous variables exercises - question 2}

# carat by price

ggplot(data = diamonds, mapping = aes(x = price, y = carat)) + 
  geom_boxplot(mapping = aes(group = cut_width(price, 1000, varwidth = TRUE)))+
     theme_minimal()+ 
   ggtitle("Carat partitioned by price")

```
#### Question 3 - Distribution 7.5.3.1 Exercises
How does the price distribution of very large diamonds compare to small diamonds? Is it as you expect, or does it surprise you?

* It looks lik the greatest variation is in the 7-20k range, which is probably the majority of the market in terms of number of transactions. Most people are buying diamonds under 20k via many retailers and for many purposes. The really big diamonds must be tied to fewer wealthy buyers or industrial buyers and perhaps that constrains the variability.  Really cheap diamonds don't vary much too, perhaps also due to being used for specific purposes in industry.

#### Question 4 - combined 7.5.3.1 Exercises
Combine two of the techniques youâ€™ve learned to visualise the combined distribution of cut, carat, and price.

```{r continuous variables exercises - question 4}

# Cut width vs cut number comparison

ggplot(data = diamonds, mapping = aes(x = price, y = carat)) + 
  geom_boxplot(mapping = aes(group = cut_width(price, 1000, varwidth = TRUE)))+
  geom_boxplot(mapping = aes(group = cut_number(carat, 20)))+
     theme_minimal()+ 
   ggtitle("Partitioned by price with carat bins overlayed")

```

#### Question 5 - Scatterplot 7.5.3.1 Exercises

Why is a scatterplot a better display than a binned plot for this case?

* It shows the outliers better, and makes it much clearer that the majority of data follows a very tight slope. In the binned plots it's fun to see individual distributions but you lose the forest for the trees.
 

```{r  scatterplot question }

ggplot(data = diamonds) +
  geom_point(mapping = aes(x = x, y = y)) +
  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11))

```

#### Need section number for this one

Overplotting becomes an issue when there are many points in the same area, and using transparency, while simple, doesn't always help. Often there are SO many points in a close area that it still doesn't do the volume of points justice. There are a few other options to help with this issue - one of them is using hexagonal bins and color scale to represent the density of points in an area. This is demonstrated below:   
   
First, load the package "hexbin":   
   
```{r}
#install.packages("hexbin") (Commented out for smoother run)
library(hexbin)
```   
   
Then, load in and format the data to be used:   
   
```{r}
library(fueleconomy)
vehicles <- vehicles
nonelectricvehicles <- subset(vehicles,vehicles$fuel!="Electricity")
```   
   
This dataset contains a massive number of vehicles with non-electric fuel sources. So when we attempt to plot in-town and highway fuel economies (both continuous variables) for all non-electric cars in the dataset, we get an overplotting issue:   
   
```{r}
library(ggplot2)
ggplot() +
  geom_point(mapping=aes(cty,hwy),data=nonelectricvehicles) +
  ggtitle("Non-Electric Vehicle Fuel Economy") +
  xlab("In-town Fuel Economy (mpg)") +
  ylab("Highway Fuel Economy (mpg)") +
  theme(plot.title = element_text(hjust = 0.5))
```   
   
We can tell that these points are overplotted because there are over 33,000 vehicles plotted in the figure, and there appear to be FAR fewer points than that (meaning a lot of them are overlapping exactly). So we can implement the hexagonal bins method of dealing with overplotting:   
   
```{r}
ggplot(data=nonelectricvehicles) +
  geom_hex(mapping = aes(cty,hwy)) +
  ggtitle("Non-Electric Vehicle Fuel Economy") +
  xlab("In-town Fuel Economy (mpg)") +
  ylab("Highway Fuel Economy (mpg)") +
  scale_fill_continuous(name="Number of Vehicles") +
  theme(plot.title = element_text(hjust = 0.5))
```   
   
This plot does a much better job of showing the distribution of fuel economies. The color scale allows the viewer to see that a much larger number of vehicles fall into a small middle range of highway and in-town fuel economies. This method will be most useful in situations similar to this one - where there are many many observations with similar values, which still need to be explored carefully, and with patterns which the creator of the graphic does not want to be obscured by the sheer volume of points.   
